# Kubernetes

Date: 06-11-2023

## Namespace (Contd.)

- In AWS, there are VPC level and non-vpc level resources
- VPC level resources i.e. attached to VPC are:
  - subnet
  - security group
  - Load balancer
- E.g. for non vpc level resources is Route53
- Even in K8s, we have namespace level and non-namespace level resources
- Few resources are attached to the namespace such as project level or cluster level resources and few are not such as nodes
- We can get the list of resources: `kubectl api-resources`
- We can identify them using the true/false value set to the resource under Namespaced column

## Pods

- Pods are smallest deployment units in K8s
- Every resource in Kubernetes has: `apiVersion`, `kind` and `metadata`

  ```yaml
  apiVersion: v1
  kind: Pod # What kind of resource is it?
  metadata:
    name: nginx # Name of the pod
    namespace: roboshop # To attach it to roboshop namespace
  ```

- All the other configuration information is specified under spec
- For e.g. when we run nginx as a docker container: `docker run -d -p 80:8080 nginx:latest`
  - Here `-p 80:8080` information is specifed under spec
- We use K8s to run the images but not docker, therefore we need `manifest.yaml` files to run the pods

  `01-pod.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod # What kind of resource is it?
  metadata:
    name: nginx # Name of the pod
    namespace: roboshop
  spec:
    containers:
    - name: simple-pod # Name of the container
      image: nginx:1.14.2
      ports:
      - containerPort: 80
  ```

**How to create pod?**

- First ensure that roboshop namespace is present
  - If not, create it using `kubectl apply -f 01-namespace.yaml`
- Then run `kubectl apply -f 01-pod.yaml`
- To get the list of pods in **roboshop** namespace: `kubectl get pods -n roboshop`
- To delete a pod: `kubectl delete -f 01-pod.yaml`

**How to login to a pod?**

- `kubectl exec -it <name-of-the-pod> -n roboshop -- bash`

### Difference between Pods and container

- 1 pod can have multiple containers
- All containers inside pod can share same storage and n/w

**Why Kubernetes offers multi-containers?**

- In ELK, we installed an agent such as filebeat on the component server which monitors `/var/log/messages` file and pushing the messages to ELK Cluster
- Similarly inside Pod, we can run multiple containers. For e.g. 1 NGiNX pod, 1 filebeat pod
- NGiNXs stores the logs to `/var/log/nginx/access.log` file and as both the containers are inside the same pod, filebeat can access this file as they share common storage and push the messages to ELK cluster
- Here **filebeat** container is referred as **side car** container i.e. offers extra functionality to the main container
- Multi containers can be used in:
  - side car
  - proxy patterns
  - init containers

**How can we create multiple containers inside a pod?**

  `02-multi-container.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod # What kind of resource is it?
  metadata:
    name: multi-container # Name of the pod
    namespace: roboshop
  spec:
    containers:
    - name: nginx
      image: nginx
    - name: sidecar
      image: almalinux:8
      command: ["sleep", "200"]
  ```

- As almalinux doesn't have a command to run for infinite time, therefore we need to explicity specify a command on our own
- The pods can created with the command: `kubectl apply -f 02-multi-container.yaml`
- Once they are created, we can login in to the pods using: `kubectl exec -it <name-of-the-pod> -c <name-of-the-container> -- bash`.
  - For e.g. `kubectl exec -it multi-container -c sidecar -- bash`
- As both these containers share the same network, we can run `curl localhost` to see the default homepage that is served by NGiNX
- To access the logs generated by NGiNX inside sidecar container, we need to perform **Volume mapping**

**How to add labels to the pods?**

  `03-labels.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: nginx # Name of the pod
    namespace: roboshop
    labels:
      environment: production
      app: nginx
  spec:
    containers:
    - name: nginx
      image: nginx
    - name: sidecar
      image: almalinux:8
      command: ["sleep", "200"]
  ```

- Labels are used in selectors and it has some functional advantage apart from filtering
- Like `docker inspect`, we can inspect a pod using: `kubectl describe pod <name-of-the-pod>`

**Difference between Labels and annotations:**

- labels can't have special char in key names, annotations can have
- labels key have some length restrictions, annotations can have more length compared to labels
- labels are used for selecting internal kubernetes resources selectors where as annotations are used in selecting external resources

  `04-annotations.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: nginx # Name of the pod
    namespace: roboshop
    labels:
      environment: production
      app: nginx
    annotations:
      com.project.name: roboshop
      com.component.name: frontend
  spec:
    containers:
    - name: nginx
      image: nginx
    - name: sidecar
      image: almalinux:8
      command: ["sleep", "200"]
  ```

**Difference between VM and Docker:**

- In VM, the resources are blocked when we create and run it even if we don't use them where as with Docker containers, it can dynamically consume te system resources
- But the disadvantage with dynamic consumption is that, it can also end up occupying complete system resources based on the task it is performing such application logs
- To overcome this, we can define resource limits i.e. soft limit and/or hardlimit:
  - Soft limit using `requests`
  - hard limit using `limits`
- To ensure that the resources are not blocking the memory and ram rather they're used by K8s to monitor the pods so that they do not end-up consuming more resources than mentioned.

  `05-resources.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: resources
  spec:
    containers:
    - name: nginx
      image: nginx
      resources:
        requests: # soft limit
          memory: "64Mi"
          cpu: "250m"
        limits: # hard limit i.e. max. limit
          memory: "128Mi"
          cpu: "500m"
  ```

- 1 CPU is 1000 milli cores, 250m -> 0.25 CPU

### Image Pull Policy

- We're publishing our images to dockerhub
- By default K8s only pulls the image from docker hub at the time of creation of the pod
- If there are any updates to the image, it will not fetch them by default
- To fetch the updates, we need to set: `imagePullPolicy: Always`

  `05-resources.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: resources
  spec:
    containers:
    - name: nginx
      image: nginx
      imagePullPolicy: Always
      resources:
        requests: # soft limit
          memory: "64Mi"
          cpu: "250m"
        limits: # hard limit i.e. max. limit
          memory: "128Mi"
          cpu: "500m"
  ```

## Environment Variables

- Defining environment variables to access them inside the container

  `06-environment.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: env-var-demo
  spec:
    containers:
    - name: env-var-demo
      image: nginx
      env:
      - name: DEMO
        value: "Hello Environment variable"
  ```

## Config map

- It is used for keeping the configuration information separate from the container as its a best practice

  `07-configmap.yaml`

  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: course-config
  data:
    COURSE: DevOps
    DURATION: 120HRS
    TRAINER: SIVAKUMAR
  ```

- To use the above defined configmap inside a pod, we need to first create it using `kubectl apply -f 07-configmap.yaml`
- To list all the configmaps: `kubectl get configmaps`
- To inspect the config map: `kubectl describe configmap <name-of-the-configmap>`. For e.g. `kubectl describe configmap course-config`
- Once the configuration is created, we can attach it to a pod by fetching the values from a configmap using:

  ```yaml
  - name: COURSE
    valueFrom:
      configMapKeyRef:
        name: course-config
        key: COURSE
  ```

  `08-config-pod.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: config-pod
  spec:
    containers:
    - name: config-pod
      image: nginx
      env:
      - name: COURSE
        valueFrom:
          configMapKeyRef:
            name: course-config
            key: COURSE
      - name: PERSON
        valueFrom:
          configMapKeyRef:
            name: course-config
            key: TRAINER
      - name: DURATION
        valueFrom:
          configMapKeyRef:
            name: course-config
            key: DURATION
  ```

- In future, if we need to change the values, we just modify it inside the ConfigMap and restart the pod i.e. delete and create the pod
- But this approach is not scalable if we have more configuration variables like 10 or even more

### How to import all configuration variables from configmap

- Instead of hardcoding all the values and assigning them to variables in the pod manifest, we can import all of them at one go
- This way we can have the pod manifest file as small as possible

  `09-import-all.yaml`

  ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: import-all
    spec:
      containers:
      - name: import-all
        image: nginx
        envFrom:
        - configMapRef:
          name: course-config
  ```

## Secret

- We can store URL information in ConfigMap but not sensitive information such as username and password
- For this purpose, we can use a `Secret` resource type
- The information that we store in Secret resource should be base64 encoded
- To encode, we can use: `echo "pavan" | base64` and to decode `echo "cGF2YW4K" | base64 --decode`
- We can define multiple resources in the same yaml file with `---` separated

  `10-secret.yaml`

  ```yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: secret-basic-auth
  data:
    username: c2l2YWt1bWFyCg==
    password: YWJjMTIzCg==
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: secret-pod
  spec:
    containers:
    - name: secret-pod
      image: nginx
      envFrom:
      - secretRef:
          name: secret-basic-auth
  ```

- Now that the pods are running, how can we allow users to access the application running inside the pod as we don't have any port mappings enabled ?
- For this purpose, we can use **Services** in K8s i.e. we can we expose the deployed pods to the external users
- The secrets information can be managed using the service available on the cloud platform
- For e.g. SecretsManager on AWS can be used to manage these credentials

## K8s Services

- There are 3 kinds of services
  1. Cluster IP
  2. NodePort
  3. Load Balancer: Only for Cloud related kubernetes
- NodePort and LoadBalancer services are used to expose the application to external users
- For NodePort service and Load Balancer service, minikube environment is not sufficient. We need to provision an actual K8s cluster

### Advantages of using a service

1. To expose the application to outside world
2. To balance the load --> at the time of deployment and replicaset
3. Serves as a **Service Mesh** as well i.e. even though the IP address changes, it gets mapped to the DNS

### Cluster IP

- Internal to cluster, not exposed to outside world, only exposed within the cluster
- We need to first create a pod and then attach it to a service
- Its important to provide the port information as well or else the service cannot attach to the pod

  `services/01-cluster-ip.yaml`

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: nginx
    labels: # These are used for selecting and filtering the pod
      environment: dev
      app: frontend
      project: roboshop
  spec:
    containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 80
          name: http-web-svc
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: nginx-service
  spec:
    # search for pods that matches the given labels and attach to it
    selector:
      environment: dev
      app: frontend
      project: roboshop
    ports:
    - name: name-of-service-port
      protocol: TCP
      port: 80 # this port belongs to service
      targetPort: http-web-svc # this port belongs to container
  ```

- Once the service is provisioned using `kubectl apply -f 01-cluster-ip.yaml`, a cluster IP is associated with the service
- We can get the service information using: `kubectl get service`
- To test the same, we can proivison a new pod using `kubectl apply -f pods/02-multi-container.yaml` and test the same using:
  - Cluster IP address: `curl <cluster-ip>` or
  - Service name: `curl nginx-service`
